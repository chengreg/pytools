#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
æ‰¹é‡ç¿»è¯‘â€œæ³¨é‡Šä¸ºä¸­æ–‡ï¼ˆè‹±æ–‡åŸæ–‡ä¿ç•™ã€ä¸­æ–‡é€è¡Œæ’å…¥ï¼‰â€ï¼Œä¸æ”¹åŠ¨æºæ–‡ä»¶ã€‚
ç›®å½•çº¦å®šï¼ˆå‡åœ¨è„šæœ¬åŒçº§ï¼‰ï¼š
- pending_sources/     æ”¾å¾…å¤„ç†çš„æºç ï¼ˆå¯åŒ…å«å­ç›®å½•ï¼‰
- translated_sources/  è¾“å‡ºå¤„ç†åçš„æºç ï¼ˆé•œåƒç›®å½•ç»“æ„ï¼‰

æ–°å¢ï¼š
- å®æ—¶è¿›åº¦æ˜¾ç¤ºï¼ˆæ•´ä½“è¿›åº¦ã€æ–‡ä»¶è¿›åº¦ã€æ‰¹æ¬¡è¿›åº¦ï¼‰
- --verbose æ˜¾ç¤ºè¯¦ç»†æ—¥å¿—
- --preview N æ˜¾ç¤ºæ¯ä¸ªæ‰¹æ¬¡ç¿»è¯‘çš„å‰ N è¡Œä¸­è‹±å¯¹ç…§é¢„è§ˆ

è¿è¡Œï¼š
  export DEEPSEEK_API_KEY="ä½ çš„å¯†é’¥" # Windows ç”¨ set æˆ– setx
  python translate_comments_with_deepseek.py --verbose --preview 3
"""

import os
import re
import json
import time
import argparse
import requests
from pathlib import Path
from typing import List, Tuple

# ========== ç›®å½•ä¸åŸºæœ¬é…ç½® ==========
SCRIPT_DIR = Path(__file__).resolve().parent
INPUT_ROOT = SCRIPT_DIR / "pending_sources"
OUTPUT_ROOT = SCRIPT_DIR / "translated_sources"

SUPPORTED_EXTS = {
    ".h", ".hpp", ".hh",
    ".cpp", ".cc", ".c", ".inl",
    ".cs", ".js", ".ts",
    ".java", ".swift",
    ".m", ".mm",
}

EXCLUDE_DIRS = {".git", ".svn", ".hg", "__pycache__", "node_modules", "build", "Binaries", "Intermediate"}

# ========== DeepSeek API é…ç½® ==========
DEFAULT_BASE_URL = os.environ.get("DEEPSEEK_BASE_URL", "https://api.deepseek.com")
DEFAULT_ENDPOINT = os.environ.get("DEEPSEEK_ENDPOINT", "/chat/completions")
DEFAULT_MODEL = os.environ.get("DEEPSEEK_MODEL", "deepseek-chat")
TIMEOUT_SEC = 60
RETRY = 3
RETRY_BACKOFF = 2.0

# æ¯æ‰¹æ¬¡é™é¢ï¼Œé¿å…ä¸Šä¸‹æ–‡è¿‡é•¿
MAX_LINES_PER_BATCH = 120
MAX_CHARS_PER_BATCH = 6000

# æ­£åˆ™ï¼šæ³¨é‡Šè¯†åˆ«
LINE_COMMENT_RE = re.compile(r'^(\s*)(//)(.*)$')
BLOCK_COMMENT_START_RE = re.compile(r'^(\s*)/\*\*?')   # /* æˆ– /**
BLOCK_COMMENT_END_RE = re.compile(r'.*\*/\s*$')
BLOCK_INNER_LINE_RE = re.compile(r'^(\s*\*)(\s?)(.*)$')  # è¯¸å¦‚ " * xxx"

# ========== æ‰“å°è¿›åº¦å·¥å…· ==========
def format_pct(numer, denom):
    pct = (numer / denom * 100.0) if denom else 100.0
    return f"{pct:6.2f}%"

def progress_bar(numer, denom, width=30):
    if denom <= 0:
        return "[" + "=" * width + "]"
    filled = int(width * numer / denom)
    return "[" + "=" * filled + ">" + "." * (width - filled - 1 if width - filled - 1 >= 0 else 0) + "]"

def print_overwrite(msg: str):
    print("\r" + msg, end="", flush=True)

def println(msg: str = ""):
    print(msg, flush=True)

# ========== æ³¨é‡Šå—æå– ==========
def split_into_comment_spans(lines: List[str]) -> List[Tuple[str, int, int]]:
    """æ‰«ææ–‡ä»¶ï¼Œè¿”å›æ‰€æœ‰æ³¨é‡ŠåŒºé—´ (kind, start_idx, end_idx)ã€‚kind in {"line","block"}"""
    spans = []
    i, n = 0, len(lines)
    while i < n:
        line = lines[i]
        # å—æ³¨é‡Š
        if BLOCK_COMMENT_START_RE.search(line):
            start = i
            j = i
            while j < n:
                if BLOCK_COMMENT_END_RE.search(lines[j]):
                    break
                j += 1
            end = min(j, n - 1)
            spans.append(("block", start, end))
            i = end + 1
            continue
        # è¿ç»­ // åˆå¹¶ä¸ºä¸€æ®µ
        if LINE_COMMENT_RE.match(line):
            start = i
            j = i + 1
            while j < n and LINE_COMMENT_RE.match(lines[j]):
                j += 1
            spans.append(("line", start, j - 1))
            i = j
            continue
        i += 1
    return spans

def build_batches_from_spans(lines: List[str], spans: List[Tuple[str, int, int]]):
    """å°†æ³¨é‡ŠåŒºé—´æŒ‰ä½“ç§¯åˆ†æ‰¹ï¼Œæ§åˆ¶æ¯æ¬¡è¯·æ±‚å¤§å°"""
    batches, cur = [], []
    chars = 0
    count = 0
    for kind, s, e in spans:
        chunk_len = sum(len(lines[k]) for k in range(s, e + 1))
        chunk_lines = e - s + 1
        if cur and (count + chunk_lines > MAX_LINES_PER_BATCH or chars + chunk_len > MAX_CHARS_PER_BATCH):
            batches.append(cur); cur = []; chars = 0; count = 0
        cur.append((kind, s, e))
        chars += chunk_len; count += chunk_lines
    if cur:
        batches.append(cur)
    return batches

# ========== DeepSeek è°ƒç”¨ ==========
def deepseek_translate(text_block: str, model: str) -> str:
    """è°ƒç”¨ DeepSeek Chat Completionsï¼Œå°†è‹±æ–‡é€è¡Œç¿»æˆä¸­æ–‡ï¼ˆè¡Œæ•°ä¸é¡ºåºä¿æŒä¸€è‡´ï¼‰"""
    url = DEFAULT_BASE_URL.rstrip("/") + DEFAULT_ENDPOINT
    api_key = os.environ.get("DEEPSEEK_API_KEY", "").strip()
    if not api_key:
        raise RuntimeError("DEEPSEEK_API_KEY æœªè®¾ç½®ã€‚")

    sys_prompt = (
        "You are a professional translator for source code comments. "
        "Translate from English to Simplified Chinese. "
        "STRICT RULES:\n"
        "1) Preserve the exact number of lines and their order.\n"
        "2) Do not merge or split lines.\n"
        "3) Do not translate code, only the given comment text.\n"
        "4) Do not add explanations or extra punctuation.\n"
        "5) Keep Unreal Engine terms consistent (Delegate, Blueprint, UObject, etc.).\n"
    )
    user_msg = "Translate each line to Simplified Chinese, same count/order.\nINPUT LINES:\n" + text_block

    payload = {
        "model": model or DEFAULT_MODEL,
        "temperature": 0.2,
        "messages": [
            {"role": "system", "content": sys_prompt},
            {"role": "user", "content": user_msg}
        ]
    }
    headers = {"Authorization": f"Bearer {api_key}", "Content-Type": "application/json"}

    for attempt in range(1, RETRY + 1):
        try:
            r = requests.post(url, headers=headers, data=json.dumps(payload), timeout=TIMEOUT_SEC)
            if r.status_code >= 400:
                raise RuntimeError(f"HTTP {r.status_code}: {r.text}")
            data = r.json()
            return data["choices"][0]["message"]["content"]
        except Exception:
            if attempt >= RETRY:
                raise
            time.sleep(RETRY_BACKOFF ** attempt)
    return ""

# ========== æ³¨é‡ŠæŠ½å–/å›å¡« ==========
def extract_comment_text_for_translation(lines: List[str], span: Tuple[str, int, int]):
    """æŠ½å–ä¸€ä¸ªæ³¨é‡ŠåŒºé—´çš„â€œå¾…ç¿»è¯‘æ–‡æœ¬â€å’Œâ€œè¡Œå‰ç¼€â€ï¼Œç”¨äºä¿æŒæ ¼å¼ã€‚"""
    kind, s, e = span
    texts, prefixes = [], []
    if kind == "line":
        for i in range(s, e + 1):
            m = LINE_COMMENT_RE.match(lines[i])
            indent, slashes, content = m.group(1), m.group(2), m.group(3)
            content = content[1:] if content.startswith(" ") else content
            texts.append(content.rstrip("\n"))
            prefixes.append(f"{indent}{slashes} ")
    else:
        for i in range(s + 1, e):  # ä»…å¤„ç†å†…éƒ¨è¡Œï¼›é¦–å°¾ /* ä¸ */ åŸæ ·ä¿ç•™
            line = lines[i].rstrip("\n")
            m2 = BLOCK_INNER_LINE_RE.match(line)
            if m2:
                star_prefix, space_opt, content = m2.group(1), m2.group(2), m2.group(3)
                pref = star_prefix + (space_opt if space_opt else " ")
                texts.append(content)
                prefixes.append(pref)
            else:
                leading = re.match(r'^(\s*)', line).group(1)
                body = line.strip()
                texts.append(body)
                prefixes.append(leading + "* ")
    return texts, prefixes

def merge_translation_back(lines: List[str], span: Tuple[str, int, int], cn_lines: List[str], prefixes: List[str]):
    """æŠŠä¸­æ–‡ç¿»è¯‘æ’å›åˆ°è‹±æ–‡æ³¨é‡Šè¡Œçš„ä¸‹ä¸€è¡Œã€‚"""
    kind, s, e = span
    if kind == "line":
        insert_offset = 0
        for idx, i in enumerate(range(s, e + 1)):
            target = i + insert_offset + 1
            lines.insert(target, prefixes[idx] + cn_lines[idx] + "\n")
            insert_offset += 1
    else:
        insert_offset = 0
        for k, i in enumerate(range(s + 1, e)):
            target = i + insert_offset + 1
            lines.insert(target, prefixes[k] + cn_lines[k] + "\n")
            insert_offset += 1

# ========== æ–‡ä»¶å¤„ç† ==========
def process_one_file(input_path: Path, output_root: Path, input_root: Path, model: str,
                     verbose: bool, preview_lines: int) -> Tuple[Path, int, int]:
    """å¤„ç†å•ä¸ªæ–‡ä»¶ï¼šè¯»å– -> ç¿»è¯‘æ³¨é‡Š -> é•œåƒå†™å…¥åˆ° output_root ä¸‹
       è¿”å›ï¼š(è¾“å‡ºæ–‡ä»¶è·¯å¾„, æ‰¹æ¬¡æ•°, æˆåŠŸæ‰¹æ¬¡æ•°)"""
    with input_path.open("r", encoding="utf-8", errors="ignore") as f:
        lines = f.readlines()

    spans = split_into_comment_spans(lines)
    batches = build_batches_from_spans(lines, spans) if spans else []
    total_batches = len(batches)
    ok_batches = 0

    if total_batches and verbose:
        println(f"  æ‰¹æ¬¡æ•°ï¼š{total_batches}")

    # ç¿»è¯‘ä¸å›å¡«
    if batches:
        all_results = {}
        for bi, batch in enumerate(batches, start=1):
            # ç»„è£…æ‰¹æ¬¡è¾“å…¥
            SEP = "\n<<<__BLOCK_SEP__>>>\n"
            INPUTS, META = [], []
            for span in batch:
                texts, prefixes = extract_comment_text_for_translation(lines, span)
                INPUTS.append("\n".join(texts) if texts else "")
                META.append((span, texts, prefixes))
            joined = SEP.join(INPUTS)

            # æ‰¹æ¬¡è¿›åº¦å±•ç¤º
            msg = f"    æ‰¹æ¬¡ {bi}/{total_batches} {progress_bar(bi-1, total_batches)} {format_pct(bi-1, total_batches)} æ­£åœ¨ç¿»è¯‘..."
            print_overwrite(msg)

            # ç¿»è¯‘
            translated = None
            if joined.strip():
                try:
                    translated = deepseek_translate(joined, model=model)
                except Exception as e:
                    if verbose:
                        println(f"\n    è¯¥æ‰¹æ¬¡è°ƒç”¨å¤±è´¥ï¼š{e}")

            # å¤„ç†ç»“æœ
            if translated is None:
                for (span, texts, prefixes) in META:
                    all_results[span] = (texts, prefixes)  # å›é€€ï¼šä¿ç•™è‹±æ–‡
            else:
                parts = translated.split("<<<__BLOCK_SEP__>>>")
                if len(parts) != len(META):
                    for (span, texts, prefixes) in META:
                        all_results[span] = (texts, prefixes)
                else:
                    ok_batches += 1
                    # å¯é€‰é¢„è§ˆ
                    if verbose and preview_lines > 0:
                        println("")  # æ¢è¡Œè½ä¸‹è¿›åº¦æç¤º
                        println(f"      âœ“ æ‰¹æ¬¡ {bi} ç¿»è¯‘å®Œæˆï¼ˆé¢„è§ˆå‰ {preview_lines} è¡Œï¼‰ï¼š")
                    for part, (span, texts, prefixes) in zip(parts, META):
                        cn_lines = [l.rstrip("\n") for l in part.splitlines()]
                        if len(cn_lines) != len(texts):
                            all_results[span] = (texts, prefixes)
                        else:
                            all_results[span] = (cn_lines, prefixes)
                            if verbose and preview_lines > 0:
                                # åªé¢„è§ˆé¦–ä¸ª span çš„å‰å‡ è¡Œï¼Œé¿å…è¿‡å¤šè¾“å‡º
                                for i, (en, cn) in enumerate(zip(texts[:preview_lines], cn_lines[:preview_lines]), start=1):
                                    println(f"        EN{i}: {en}")
                                    println(f"        CN{i}: {cn}")
                                # åªå¯¹ç¬¬ä¸€ä¸ª span é¢„è§ˆä¸€æ¬¡
                                preview_lines = 0
                    # æ›´æ–°æ‰¹æ¬¡è¿›åº¦åˆ°â€œå·²å®Œæˆâ€
            msg_done = f"    æ‰¹æ¬¡ {bi}/{total_batches} {progress_bar(bi, total_batches)} {format_pct(bi, total_batches)} å®Œæˆ"
            print_overwrite(msg_done)
            println("")  # æ¢è¡Œ

        # å›å¡«ï¼ˆä»åå¾€å‰æ’å…¥ï¼Œé¿å…ç´¢å¼•é”™ä½ï¼‰
        for span in sorted(spans, key=lambda x: x[1], reverse=True):
            texts, prefixes = all_results.get(span, (None, None))
            if not texts or not prefixes:
                continue
            merge_translation_back(lines, span, texts, prefixes)

    # è¾“å‡ºï¼ˆé•œåƒ pending_sources çš„ç›¸å¯¹è·¯å¾„ï¼‰
    rel_path = input_path.relative_to(input_root)
    out_path = output_root / rel_path.parent / (rel_path.stem + "_zh_annotated" + rel_path.suffix)
    out_path.parent.mkdir(parents=True, exist_ok=True)
    with out_path.open("w", encoding="utf-8") as fo:
        fo.writelines(lines)

    return out_path, total_batches, ok_batches

# ========== ç›®å½•æ‰«æ ==========
def should_skip_dir(p: Path) -> bool:
    return p.name in EXCLUDE_DIRS

def should_process_file(p: Path) -> bool:
    return p.suffix.lower() in SUPPORTED_EXTS and p.is_file()

# ========== ä¸»æµç¨‹ ==========
def main():
    ap = argparse.ArgumentParser(description="Translate only code comments to Chinese with realtime progress.")
    ap.add_argument("--verbose", action="store_true", help="æ˜¾ç¤ºæ›´è¯¦ç»†çš„ç¿»è¯‘æ—¥å¿—ä¸æ‰¹æ¬¡è¿›åº¦")
    ap.add_argument("--preview", type=int, default=2, help="æ¯æ‰¹ç¿»è¯‘é¢„è§ˆå‰ N è¡Œï¼ˆé»˜è®¤2ï¼Œ0ä¸ºä¸é¢„è§ˆï¼‰")
    args = ap.parse_args()

    if not INPUT_ROOT.exists():
        println(f"âš ï¸ æœªæ‰¾åˆ°å¾…å¤„ç†ç›®å½•ï¼š{INPUT_ROOT}")
        println("è¯·åœ¨è„šæœ¬åŒçº§åˆ›å»º pending_sources/ï¼Œå¹¶å°†æºç æ–‡ä»¶æ”¾å…¥å…¶ä¸­ã€‚")
        return

    OUTPUT_ROOT.mkdir(parents=True, exist_ok=True)

    files = []
    for path in INPUT_ROOT.rglob("*"):
        if path.is_dir():
            if should_skip_dir(path):
                continue
        else:
            if should_process_file(path):
                files.append(path)

    if not files:
        println(f"ğŸ¤” åœ¨ {INPUT_ROOT} æœªå‘ç°å¯å¤„ç†çš„æºç æ–‡ä»¶ã€‚æ”¯æŒåç¼€ï¼š{', '.join(sorted(SUPPORTED_EXTS))}")
        return

    total_files = len(files)
    println(f"ğŸ”§ å¾…å¤„ç†æ–‡ä»¶æ•°ï¼š{total_files}")
    ok_files, fail_files = 0, 0
    total_batches_all = 0
    ok_batches_all = 0

    for idx, f in enumerate(files, start=1):
        rel = f.relative_to(INPUT_ROOT)
        head = f"[{idx}/{total_files}] {rel}"
        print_overwrite(f"{head} {progress_bar(idx-1, total_files)} {format_pct(idx-1, total_files)} æ­£åœ¨å¤„ç†...")
        try:
            if args.verbose:
                println("")  # æ¢è¡Œï¼Œå¼€å§‹è¯¦ç»†æ—¥å¿—
                println(f"ğŸ“„ æ–‡ä»¶ï¼š{rel}")

            out, total_batches, ok_batches = process_one_file(
                f, OUTPUT_ROOT, INPUT_ROOT, DEFAULT_MODEL, args.verbose, args.preview
            )
            total_batches_all += total_batches
            ok_batches_all += ok_batches
            ok_files += 1
            print_overwrite(f"{head} {progress_bar(idx, total_files)} {format_pct(idx, total_files)} å®Œæˆ -> {out.relative_to(OUTPUT_ROOT)}")
            println("")
        except Exception as e:
            fail_files += 1
            print_overwrite(f"{head} å¤±è´¥ï¼š{e}")
            println("")

    println("\n================ ç»Ÿè®¡ =================")
    println(f"æ–‡ä»¶ï¼šæˆåŠŸ {ok_files} / {total_files}ï¼Œå¤±è´¥ {fail_files}")
    if total_batches_all:
        println(f"æ‰¹æ¬¡ï¼ˆæ³¨é‡Šå—ç¿»è¯‘ï¼‰ï¼šæˆåŠŸ {ok_batches_all} / {total_batches_all}ï¼ˆ{format_pct(ok_batches_all, total_batches_all).strip()}ï¼‰")
    println(f"è¾“å‡ºç›®å½•ï¼š{OUTPUT_ROOT}")

if __name__ == "__main__":
    main()
